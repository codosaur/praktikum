{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим колонку `text_lemmatized`. В итоге в ней окажется лемматизированный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_lemmatized'] = data['text'].values.astype('U')\n",
    "\n",
    "# Сначала оставим в каждом твите только буквы и апостроф \"'\", чтобы сохранить I'm и т.п.\n",
    "data['text_lemmatized'] = data['text_lemmatized'].apply(lambda x: re.sub(r'[^a-zA-Z\\']', ' ', x))\n",
    "\n",
    "# Затем переведем весь текст в нижний регистр\n",
    "data['text_lemmatized'] = data['text_lemmatized'].str.lower()\n",
    "data['text_lemmatized'] = data['text_lemmatized'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE\n",
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "AFTER\n",
      "explanation why the edits made under my username hardcore metallica fan were reverted they weren't vandalisms just closure on some gas after i voted at new york dolls fac and please don't remove the template from the talk page since i'm retired now\n"
     ]
    }
   ],
   "source": [
    "print('BEFORE')\n",
    "print(data.loc[0, 'text'])\n",
    "print('AFTER')\n",
    "print(data.loc[0, 'text_lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно **проведем лемматизацию**. Использую готовый пример лемматизации **с использованием WordNetLemmatizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dmitry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dmitry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dmitry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 9min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize a Sentence with the appropriate POS tag\n",
    "data['text_lemmatized'] = data['text_lemmatized'].apply(\n",
    "    lambda x: ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE\n",
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "AFTER\n",
      "explanation why the edits make under my username hardcore metallica fan be revert they be n't vandalism just closure on some gas after i vote at new york doll fac and please do n't remove the template from the talk page since i 'm retire now\n"
     ]
    }
   ],
   "source": [
    "print('BEFORE')\n",
    "print(data.loc[0, 'text'])\n",
    "print('AFTER')\n",
    "print(data.loc[0, 'text_lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119678, 3)\n",
      "(39893, 3)\n"
     ]
    }
   ],
   "source": [
    "# Разделим данные на 2 выборки\n",
    "train, test = train_test_split(data, test_size=0.25, random_state=RANDOM_STATE)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала попробуем применить простой мешок слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tfidf = CountVectorizer()\n",
    "tfidf_train = count_tfidf.fit_transform(train['text_lemmatized'])\n",
    "tfidf_test = count_tfidf.transform(test['text_lemmatized'])\n",
    "\n",
    "model = LogisticRegression(random_state=RANDOM_STATE)\n",
    "model.fit(tfidf_train, train['toxic'])\n",
    "pred = model.predict(tfidf_test)\n",
    "f1_score1 = f1_score(test['toxic'], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7249010699105967\n"
     ]
    }
   ],
   "source": [
    "print(f1_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат неплохой, но пока не 0,75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним кодирование с TD-IDF со следующими параметрами:\n",
    "1. без использования/с использованием стоп-слов\n",
    "2. 1/2/3-грамы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dmitry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|ngrams              |Стоп-слова          |Признаков           |f1_score            |\n",
      "|         1          |<не выбраны>        |              128700|            0.756341|\n",
      "|         1          |<выбраны>           |              128560|            0.743291|\n",
      "|         2          |<не выбраны>        |             1535661|            0.486866|\n",
      "|         2          |<выбраны>           |             1862025|            0.227723|\n",
      "|         3          |<не выбраны>        |             4043404|            0.156880|\n",
      "|         3          |<выбраны>           |             3171497|            0.016019|\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_grid = {\n",
    "    'ngram_range': [(1,1), (2,2), (3,3)],\n",
    "    'stop_words': [None, stop_words_set]\n",
    "}\n",
    "\n",
    "# Результаты на каждой итерации будем сохранять в переменные\n",
    "print('|{:<20}|{:<20}|{:<20}|{:<20}|'.format('ngrams', 'Стоп-слова', 'Признаков', 'f1_score'))\n",
    "\n",
    "for ngram_range in params_grid['ngram_range']:\n",
    "    for stop_words in params_grid['stop_words']:\n",
    "        \n",
    "        count_tfidf = TfidfVectorizer(stop_words=stop_words, ngram_range=ngram_range)\n",
    "        tfidf_train = count_tfidf.fit_transform(train['text_lemmatized'])\n",
    "        tfidf_test = count_tfidf.transform(test['text_lemmatized'])\n",
    "        \n",
    "        model = LogisticRegression(random_state=RANDOM_STATE)\n",
    "        model.fit(tfidf_train, train['toxic'])\n",
    "        pred = model.predict(tfidf_test)\n",
    "        f1_score1 = f1_score(test['toxic'], pred)\n",
    "        \n",
    "        print('|{:^20}|{:<20}|{:>20}|{:>20.6f}|'.format(ngram_range[0], f'{\"<не выбраны>\" if stop_words == None else \"<выбраны>\"}', tfidf_train.shape[1], f1_score1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы достигли заданной величины метрики F1. \n",
    "\n",
    "**Лучший результат** был получен при использовании **униграм без использования словаря стоп-слов** (0,756341).\n",
    "\n",
    "Интересно, что качество модели **ухудшается при росте количества грам**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
